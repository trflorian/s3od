import torch
from diffusers import FluxPipeline
from diffusers.models import FluxTransformer2DModel
import logging
import numpy as np
from typing import Dict, Any, Optional, List

from synth_sod.data_generation.concept_attention.flux_with_concept_attention_pipeline import (
    FluxWithConceptAttentionPipeline,
)
from synth_sod.data_generation.concept_attention.flux_dit_with_concept_attention import (
    FluxTransformer2DModelWithConceptAttention,
)


def optimize(pipe: FluxPipeline, compile: bool = True) -> FluxPipeline:
    # fuse QKV projections in Transformer and VAE
    pipe.vae.fuse_qkv_projections()

    # switch memory layout to Torch's preferred, channels_last
    pipe.transformer.to(memory_format=torch.channels_last)
    pipe.vae.to(memory_format=torch.channels_last)

    if not compile:
        return pipe

    # set torch compile flags
    config = torch._inductor.config
    config.disable_progress = False  # show progress bar
    config.conv_1x1_as_mm = True  # treat 1x1 convolutions as matrix muls
    # adjust autotuning algorithm
    config.coordinate_descent_tuning = True
    config.coordinate_descent_check_all_directions = True
    config.epilogue_fusion = False  # do not fuse pointwise ops into matmuls

    # tag the compute-intensive modules, the Transformer and VAE decoder, for compilation
    pipe.transformer = torch.compile(
        pipe.transformer, mode="max-autotune", fullgraph=True
    )
    pipe.vae.decode = torch.compile(
        pipe.vae.decode, mode="max-autotune", fullgraph=True
    )

    # trigger torch compilation
    print("ðŸ”¦ running torch compiliation (may take up to 20 minutes)...")

    pipe(
        "dummy prompt to trigger torch compilation",
        output_type="pil",
        num_inference_steps=50,  # use ~50 for [dev], smaller for [schnell]
    ).images[0]

    print("ðŸ”¦ finished torch compilation")

    return pipe


def get_pipeline(
    model_path: str,
    lora_path: Optional[str] = None,
    enable_concept_attention: bool = True,
):
    """Get FLUX pipeline with optional concept attention and LoRA."""

    if enable_concept_attention:
        # Load transformer with concept attention
        transformer = FluxTransformer2DModelWithConceptAttention.from_pretrained(
            model_path, torch_dtype=torch.bfloat16, subfolder="transformer"
        )

        # Create pipeline with concept attention
        pipe = FluxWithConceptAttentionPipeline.from_pretrained(
            model_path, transformer=transformer, torch_dtype=torch.bfloat16
        )
    else:
        # Regular FLUX pipeline
        pipe = FluxPipeline.from_pretrained(model_path, torch_dtype=torch.bfloat16)

    if lora_path:
        pipe.load_lora_weights(lora_path)
        logging.info(f"Loaded LoRA weights from {lora_path}")

    # Enable optimizations
    pipe.enable_vae_slicing()
    pipe.enable_vae_tiling()

    logging.info(f"Pipeline loaded from {model_path}")
    return pipe


class FluxImageGeneratorWithFeatures:
    """Enhanced FLUX image generator that returns features for mask prediction."""

    def __init__(
        self,
        model_path: str,
        lora_path: Optional[str] = None,
        num_inference_steps: int = 28,
        guidance_scale: float = 3.5,
        feature_layers: List[int] = [0, 1, 2, 3],
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
    ):
        self.device = device
        self.num_inference_steps = num_inference_steps
        self.guidance_scale = guidance_scale
        self.feature_layers = feature_layers

        # Load pipeline with concept attention
        self.pipeline = get_pipeline(
            model_path, lora_path, enable_concept_attention=True
        )
        self.pipeline.to(device)

        # Get reference to transformer for feature extraction
        self.transformer = self.pipeline.transformer

        logging.info("FluxImageGeneratorWithFeatures initialized")

    def _setup_scheduler(self, height: int, width: int):
        """Setup scheduler for specific image dimensions."""
        image_seq_len = (height // 16) * (width // 16)
        base_seq_len, max_seq_len = 256, 4096
        base_shift, max_shift = 0.5, 1.15
        m = (max_shift - base_shift) / (max_seq_len - base_seq_len)
        b = base_shift - m * base_seq_len
        mu = max(base_shift, min(max_shift, image_seq_len * m + b))

        self.pipeline.scheduler.set_timesteps(50, device=self.device, mu=mu)
        return self.pipeline.scheduler.timesteps

    def _extract_features_from_result(self, result) -> Dict[str, Any]:
        """Extract transformer features and concept maps from generation result."""
        # Process concept attention maps
        concept_maps = {}
        if hasattr(result, "concept_attention_maps") and result.concept_attention_maps:
            concept_maps_batch = result.concept_attention_maps[0]

            if len(concept_maps_batch) > 0:
                category_map = concept_maps_batch[0]  # First concept
                concept_maps["category"] = torch.from_numpy(
                    np.array(category_map, dtype=np.float32)
                ).float()

            if len(concept_maps_batch) > 1:
                background_map = concept_maps_batch[1]  # Background
                concept_maps["background"] = torch.from_numpy(
                    np.array(background_map, dtype=np.float32)
                ).float()

        # Process transformer features
        _, transformer_features = self.transformer.get_features()
        processed_features = []

        for layer_idx in self.feature_layers:
            if layer_idx < len(transformer_features):
                feat = transformer_features[layer_idx]
                # feat shape: [B=1, H*W, C=3072]
                feat_np = feat.detach().cpu().float().numpy()[0]  # Remove batch dim

                # Channel compression: 3072 -> 768
                seq_len, C = feat_np.shape
                new_C = C // 4
                feat_reshaped = feat_np[:, : new_C * 4].reshape(seq_len, new_C, 4)
                feat_compressed = feat_reshaped.mean(axis=2)

                processed_features.append(
                    torch.from_numpy(feat_compressed.astype(np.float32))
                )

        # Clear stored features
        self.transformer.stored_features.clear()

        return {
            "transformer_features": processed_features,
            "concept_maps": concept_maps,
        }

    def generate_with_features(
        self, prompt: str, tag: str, width: int, height: int
    ) -> Dict[str, Any]:
        """
        Generate image with FLUX features for mask prediction.

        Args:
            prompt: Text prompt for image generation
            tag: Class tag for concept attention (e.g., "dog", "car")
            width: Image width
            height: Image height

        Returns:
            Dictionary with 'image', 'transformer_features', and 'concept_maps'
        """
        # Setup scheduler
        timesteps = self._setup_scheduler(height, width)

        # Define concepts for attention
        concepts = [tag, "background"]

        # Generate image with concept attention
        concept_attention_kwargs = {
            "concepts": concepts,
            "timesteps": list(range(25, 28)),  # Last 3 timesteps
            "layers": list(range(18)),
        }

        result = self.pipeline(
            prompt=prompt,
            height=height,
            width=width,
            num_inference_steps=self.num_inference_steps,
            guidance_scale=self.guidance_scale,
            concept_attention_kwargs=concept_attention_kwargs,
            generator=torch.Generator(self.device).manual_seed(42),
        )

        generated_image = result.images[0]

        # Extract features from generation
        features_data = self._extract_features_from_result(result)

        # Clear cache
        torch.cuda.empty_cache()

        return {
            "image": generated_image,
            "transformer_features": features_data["transformer_features"],
            "concept_maps": features_data["concept_maps"],
        }

    def generate(self, prompt: str, width: int, height: int):
        """Backward compatibility: generate image only."""
        return self.pipeline(
            prompt,
            output_type="pil",
            num_inference_steps=self.num_inference_steps,
            width=width,
            height=height,
            max_sequence_length=512,
        ).images[0]
